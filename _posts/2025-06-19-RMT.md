---
title: Random Matrix Theory for High-Dimensional Covariance Estimation: From Classical Results to Concentration of Measure
mathjax: true
---

This report provides a comprehensive survey of random matrix theory (RMT) techniques for analyzing sample covariance matrices in high-dimensional statistical settings. We begin by examining the limitations of classical covariance estimation when the dimension $p$ is comparable to or exceeds the sample size $n$, where traditional asymptotic guarantees fail. We present the foundational Marčenko–Pastur law, which characterizes the limiting spectral distribution of sample covariance matrices under i.i.d. Gaussian assumptions, and discuss its extensions through the Silverstein–Bai theorem to more general covariance structures.

A key focus of this report is the significant generalization achieved through concentration of measure theory. We demonstrate how the restrictive assumption of entrywise independence in classical RMT can be relaxed by requiring only that the data vectors exhibit concentration properties. Specifically, we show that concentrated random vectors—including those arising from Lipschitz transformations of Gaussian vectors—satisfy the key structural properties needed for spectral analysis, thereby extending the applicability of RMT to realistic data-generating processes in machine learning.

This survey is primarily based on the comprehensive framework established in Couillet and Debbah (2022) [^1].

## Introduction

Random Matrix Theory provides a versatile framework for analyzing and improving classical machine learning methods in high-dimensional settings. Consider the following setup: let $\mathbf{X} = [\mathbf{x}_1, \dots, \mathbf{x}_n] \in \mathbb{R}^{p \times n}$ be a data matrix whose columns $\mathbf{x}_i$ are independent samples from $\mathcal{N}(0, \mathbf{C})$, and let the maximum likelihood estimator of the covariance be $\hat{\mathbf{C}} = \frac{1}{n}\mathbf{X}\mathbf{X}^T$. 

In the classical regime, where $n \to \infty$ with fixed $p$, the law of large numbers implies 
$$
\|\hat{\mathbf{C}} - \mathbf{C}\| \xrightarrow{\mathrm{a.s.}} 0,
$$ 
i.e., $\hat{\mathbf{C}}$ converges almost surely to $\mathbf{C}$ in operator norm.

Modern datasets, however, often have $p$ comparable to or larger than $n$. Suppose $p = \mathcal{O}(n^d)$ for some $d > 0$. Although concentration inequalities still guarantee $\|\hat{\mathbf{C}} - \mathbf{C}\|_{\infty} \xrightarrow{\mathrm{a.s.}} 0$, operator-norm convergence fails when $p > n$ since $\hat{\mathbf{C}}$ becomes singular and cannot approximate a full-rank $\mathbf{C}$. This is critical because many statistical procedures (e.g., regression, classification) depend on the spectral properties of $\hat{\mathbf{C}}$, and without spectral-norm consistency, we lose control over eigenvalues and eigenvectors.

To overcome this, one studies the asymptotic spectral distribution of $\hat{\mathbf{C}}$. A cornerstone result is the Marčenko–Pastur law [^2]: when $\mathbf{C} = \mathbf{I}_p$ and $n, p \to \infty$ with $p/n \to c \in (0, \infty)$, the empirical spectral distribution 
$$
\mu_p = \frac{1}{p} \sum_{i=1}^p \delta_{\lambda_i(\hat{\mathbf{C}})}
$$ 
converges to the deterministic measure  
$$
\mu(dx) = (1 - c^{-1})_+\,\delta_0(x) + \frac{1}{2\pi c x} \sqrt{(x - E_-)^+ (E_+ - x)^+}\,dx,
$$
where $E_{\pm} = (1 \pm \sqrt{c})^2$ and $(x)^+ = \max(x, 0)$. This law precisely describes the limiting behavior of the eigenvalues of $\hat{\mathbf{C}}$, providing a rigorous foundation for high-dimensional analysis.

## Preliminaries

We start with basic definitions:

### Definition: Resolvent

For a symmetric matrix $\mathbf{M} \in \mathbb{R}^{n \times n}$, the **resolvent** $\mathbf{Q}_\mathbf{M}(z)$ is defined, for $z \in \mathbb{C}$ not an eigenvalue of $\mathbf{M}$, as
$$
\mathbf{Q}_\mathbf{M}(z) \coloneqq (\mathbf{M} - z\mathbf{I}_n)^{-1}.
$$

### Definition: Empirical Spectral Measure

For a symmetric matrix $\mathbf{M} \in \mathbb{R}^{n \times n}$, the **empirical spectral measure** $\mu_\mathbf{M}$ is defined as
$$
\mu_\mathbf{M} \coloneqq \frac{1}{n} \sum_{i=1}^{n} \delta_{\lambda_i(\mathbf{M})},
$$
where $\lambda_1(\mathbf{M}), \ldots, \lambda_n(\mathbf{M})$ are the eigenvalues of $\mathbf{M}$.

### Definition: Stieltjes Transform

For a real probability measure $\mu$, the **Stieltjes transform** $m_\mu(z)$ is defined, for all $z \in \mathbb{C} \setminus \operatorname{supp}(\mu)$, as
$$
m_\mu(z) \coloneqq \int \frac{1}{t - z} \, \mu(dt).
$$

### Theorem: Inverse Stieltjes Transform

Let $a, b$ be continuity points of the probability measure $\mu$. Then:

- $\mu([a, b]) = \frac{1}{\pi} \lim_{y \downarrow 0} \int_a^b \Im[m_\mu(x + iy)] \, dx$
- If $\mu$ admits a density $f$ at $x$, then 
  $$
  f(x) = \frac{1}{\pi} \lim_{y \downarrow 0} \Im[m_\mu(x + iy)]
  $$
- If $\mu$ has an isolated point mass at $x$, then
  $$
  \mu(\{x\}) = \lim_{y \downarrow 0} ( -iy \, m_\mu(x + iy) )
  $$

---

These definitions form the basis for analyzing eigenvalue distributions. For instance,

$$
m_{\mu_\mathbf{M}}(z) = \frac{1}{n} \sum_{i=1}^{n} \int \frac{\delta_{\lambda_i(\mathbf{M})}(t)}{t - z} = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{\lambda_i(\mathbf{M}) - z} = \frac{1}{n} \operatorname{tr} \mathbf{Q}_\mathbf{M}(z).
$$

Combining this with the inverse Stieltjes transform yields a bridge between $\mathbf{Q}_\mathbf{M}$ and $\mu_\mathbf{M}$. Using Cauchy's integral formula, for any contour $\Gamma$ and any $f$ analytic in a neighborhood of $\operatorname{supp}(\mu_\mathbf{M}) \cap \Gamma^\circ$,

$$
\frac{1}{n} \sum_{\lambda_i(\mathbf{M}) \in \Gamma^\circ} f(\lambda_i(\mathbf{M})) = -\frac{1}{2\pi i} \oint_{\Gamma} f(z) m_{\mu_{\mathbf{M}}}(z) \, dz.
$$

More generally, for the spectral decomposition $\mathbf{M} = \mathbf{U} \mathbf{\Lambda} \mathbf{U}^T$, we have

$$
\mathbf{U} f(\mathbf{\Lambda}; \Gamma) \mathbf{U}^T = -\frac{1}{2\pi i} \oint_{\Gamma} f(z) \mathbf{Q}_\mathbf{M}(z) \, dz,
$$

for $f$ analytic in a neighborhood of $\Gamma$ and its interior $\Gamma^\circ$, where

$$
f(\mathbf{\Lambda}; \Gamma) \coloneqq \operatorname{diag}\left(f(\lambda_i(\mathbf{M})) \cdot \mathbf{1}_{\lambda_i(\mathbf{M}) \in \Gamma^\circ}\right).
$$

This enables the analysis of eigenvector projections. With $\mathbf{U} = [\mathbf{u}_1, \ldots, \mathbf{u}_n]$, we get

$$
\sum_{\lambda_i(\mathbf{M}) \in \Gamma^\circ} |\mathbf{v}^T \mathbf{u}_i|^2 = -\frac{1}{2\pi i} \oint_{\Gamma} \mathbf{v}^T \mathbf{Q}_\mathbf{M}(z) \mathbf{v} \, dz.
$$

Thus, the resolvent $\mathbf{Q}_\mathbf{M}$ captures scalar observations of the eigenspectrum through linear functionals: $f(\lambda_i(\mathbf{M}))$ and $|\mathbf{v}^T \mathbf{u}_i|$ are accessible from $\frac{1}{n} \operatorname{tr} \mathbf{Q}_\mathbf{M}$ and $\mathbf{v}^T \mathbf{Q}_\mathbf{M} \mathbf{v}$, respectively.

Early approaches primarily focused on the limit of $m_{\mu_\mathbf{M}}(z)$. However, such a limit exists only for highly regular matrices $\mathbf{M}$. Moreover, $m_{\mu_\mathbf{M}}(z) = \frac{1}{n} \operatorname{tr} \mathbf{Q}_\mathbf{M}$ discards subspace information about eigenvectors encoded in the resolvent $\mathbf{Q}_\mathbf{M}$. To address this, modern methods introduce **deterministic equivalents**—matrices that yield asymptotically the same scalar observables as random ones.

### Definition: Deterministic Equivalent

We say that $\bar{\mathbf{Q}} \in \mathbb{R}^{n \times n}$ is a **deterministic equivalent** for the symmetric random matrix $\mathbf{Q} \in \mathbb{R}^{n \times n}$ if, for any deterministic matrix $\mathbf{A}$ with $\|\mathbf{A}\| = 1$ and vectors $\mathbf{a}, \mathbf{b}$ with $\|\mathbf{a}\|_2 = \|\mathbf{b}\|_2 = 1$, as $n \to \infty$, the following hold:

$$
\frac{1}{n} \operatorname{tr} \mathbf{A} (\mathbf{Q} - \bar{\mathbf{Q}}) \xrightarrow{a.s.} 0, \quad \mathbf{a}^T (\mathbf{Q} - \bar{\mathbf{Q}}) \mathbf{b} \xrightarrow{a.s.} 0.
$$

We write this as $\mathbf{Q} \leftrightarrow \bar{\mathbf{Q}}$.

The notion of equivalence can be extended to random matrices by requiring $\| \mathbb{E}[\mathbf{Q} - \bar{\mathbf{Q}}] \| \to 0$, making it an equivalence relation over both deterministic and random matrices.

---

[^1]: Couillet, R., & Debbah, M. (2022). *Random Matrix Methods for Wireless Communications*. Cambridge University Press.

[^2]: Marčenko, V. A., & Pastur, L. A. (1967). Distribution of eigenvalues for some sets of random matrices. *Mathematics of the USSR-Sbornik*, 1(4), 457.
